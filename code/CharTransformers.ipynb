{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c930d69b",
      "metadata": {},
      "source": [
        "# CharTransformers: next character prediction using Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "125af93c-648b-4394-a8fc-80d6f82d0a8a",
      "metadata": {
        "id": "125af93c-648b-4394-a8fc-80d6f82d0a8a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e49efb",
      "metadata": {},
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5e0fe960-0a13-41a5-950d-7f0bac62b182",
      "metadata": {
        "id": "5e0fe960-0a13-41a5-950d-7f0bac62b182"
      },
      "outputs": [],
      "source": [
        "file_path = \"../data/alpaca_data_cleaned_subset.json\"\n",
        "context_length = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "55607749-deef-492d-8913-6f03363170af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55607749-deef-492d-8913-6f03363170af",
        "outputId": "ce70f6f6-b69d-4a22-d19d-5d61ffd8f084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 19237\n",
            "Example input shape: (10,)\n",
            "output shape: (19237,)\n"
          ]
        }
      ],
      "source": [
        "def read_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def preprocess_data(data):\n",
        "    sequences = []\n",
        "    for item in data:\n",
        "        sequence = list(item['instruction']) + ['\\n'] + list(item['input']) + ['\\n'] + list(item['output']) + ['[EOS]']\n",
        "        sequences.append(sequence)\n",
        "    return sequences\n",
        "\n",
        "valid_chars = list('abcdefghijklmnopqrstuvwxyz ?!.,\\n')\n",
        "def preprocess_data_simple(data):\n",
        "    sequences = []\n",
        "    for item in data:\n",
        "        instruction_alpha = ''.join([char for char in item['instruction'].lower() if char in valid_chars])\n",
        "        input_alpha = ''.join([char for char in item['input'].lower() if char in valid_chars])\n",
        "        output_alpha = ''.join([char for char in item['output'].lower() if char in valid_chars])\n",
        "\n",
        "        sequence = list(instruction_alpha) + ['\\n'] + list(input_alpha) + ['\\n'] + list(output_alpha) + ['[EOS]']\n",
        "        sequences.append(sequence)\n",
        "    return sequences\n",
        "\n",
        "def create_vocab(sequences):\n",
        "    chars = [char for seq in sequences for char in seq]\n",
        "    return sorted(set(chars))\n",
        "\n",
        "def one_hot_encode(sequence, char_to_idx):\n",
        "    one_hot = np.zeros((len(sequence), len(char_to_idx)), dtype=np.int32)\n",
        "    for i, char in enumerate(sequence):\n",
        "        one_hot[i, char_to_idx[char]] = 1\n",
        "    return one_hot\n",
        "\n",
        "def numerical_encode(sequence, char_to_idx):\n",
        "    numerical_encoding = np.zeros(len(sequence), dtype=np.int32)\n",
        "    for i, char in enumerate(sequence):\n",
        "        numerical_encoding[i] = char_to_idx[char]\n",
        "    return numerical_encoding\n",
        "\n",
        "def create_training_examples(sequences, char_to_idx, input_length=10):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        encoded_seq = one_hot_encode(seq, char_to_idx)\n",
        "        numerical_encode_seq = numerical_encode(seq, char_to_idx)\n",
        "        total_chars = len(seq)\n",
        "\n",
        "        for i in range(total_chars - input_length):\n",
        "            x.append(numerical_encode_seq[i:i+input_length])\n",
        "            y.append(numerical_encode_seq[i+input_length])\n",
        "\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "# Main script\n",
        "data = read_json(file_path)\n",
        "#sequences = preprocess_data(data)\n",
        "sequences = preprocess_data_simple(data)\n",
        "\n",
        "vocab = create_vocab(sequences)\n",
        "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "\n",
        "X, Y = create_training_examples(sequences, char_to_idx, input_length=context_length)\n",
        "X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "print(f\"Number of training examples: {X.shape[0]}\")\n",
        "print(f\"Example input shape: {X[0].shape}\")\n",
        "print(f\"output shape: {Y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "423ddce1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: ' ',\n",
              " 2: ',',\n",
              " 3: '.',\n",
              " 4: '?',\n",
              " 5: '[EOS]',\n",
              " 6: 'a',\n",
              " 7: 'b',\n",
              " 8: 'c',\n",
              " 9: 'd',\n",
              " 10: 'e',\n",
              " 11: 'f',\n",
              " 12: 'g',\n",
              " 13: 'h',\n",
              " 14: 'i',\n",
              " 15: 'j',\n",
              " 16: 'k',\n",
              " 17: 'l',\n",
              " 18: 'm',\n",
              " 19: 'n',\n",
              " 20: 'o',\n",
              " 21: 'p',\n",
              " 22: 'q',\n",
              " 23: 'r',\n",
              " 24: 's',\n",
              " 25: 't',\n",
              " 26: 'u',\n",
              " 27: 'v',\n",
              " 28: 'w',\n",
              " 29: 'x',\n",
              " 30: 'y',\n",
              " 31: 'z'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idx_to_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "AGWT8b5YeCNx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGWT8b5YeCNx",
        "outputId": "ceab3061-a515-4eda-d00a-7f89b26c33ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((19237, 10), (19237,))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape, Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ytteIh2vhrtQ",
      "metadata": {
        "id": "ytteIh2vhrtQ"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XLykUMHL9Cfa",
      "metadata": {
        "id": "XLykUMHL9Cfa"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "79a26f21",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "Epoch 1/20:  52%|█████▏    | 252/481 [00:17<00:23,  9.78it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the Positional Encoding class\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, 2 * d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        self.positional_encoding = nn.Parameter(torch.sin(position * div_term), requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pos_encoding = self.positional_encoding[:x.size(1), :x.size(2)].unsqueeze(0).expand(x.size(0), -1, -1)\n",
        "        x = x + pos_encoding\n",
        "        return x\n",
        "\n",
        "# Define the Transformer model for sequence-to-sequence prediction\n",
        "class CharTransformer(nn.Module):\n",
        "    def __init__(self, input_size, output_size, embed_size, num_heads, num_layers):\n",
        "        super(CharTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embed_size)\n",
        "        self.pos_encoder = PositionalEncoding(d_model=embed_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x, x)\n",
        "\n",
        "        # Take the representation of the last token from each sequence\n",
        "        x_last_token = x[:, -1, :]\n",
        "\n",
        "        output = self.fc(x_last_token)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Define a custom dataset\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_vector = self.X[idx]\n",
        "        target_index = self.Y[idx]\n",
        "        return input_vector, target_index\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 32 # size of the vocabulary, specifically the number of unique characters in your dataset\n",
        "output_size = 32\n",
        "embed_size = 64\n",
        "num_heads = 2 # 2\n",
        "num_layers = 2 # 2\n",
        "batch_size = 32\n",
        "learning_rate = 0.001 # 0.01\n",
        "weight_decay = 1e-5  # Adjust the weight decay value as needed\n",
        "num_epochs = 20 # 10\n",
        "dim_feedforward = 2048\n",
        "\n",
        "# Create model, loss, and optimizer\n",
        "model = CharTransformer(input_size, output_size, embed_size, num_heads, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Create the dataset and DataLoader\n",
        "dataset = CharDataset(X_train, Y_train)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()  # Record the start time\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    for batch in tqdm(dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
        "        inputs, targets = batch\n",
        "\n",
        "        # Convert inputs to PyTorch tensor\n",
        "        input_tensor = torch.as_tensor(inputs, dtype=torch.long).clone().detach()\n",
        "        target_tensor = torch.as_tensor(targets, dtype=torch.long).clone().detach()\n",
        "\n",
        "        # Add batch and sequence dimensions\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_tensor)\n",
        "\n",
        "        # Check if batch sizes match before calculating the loss\n",
        "        assert outputs.size(0) == target_tensor.size(0), \"Batch sizes do not match\"\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs, target_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy on this batch\n",
        "        # Get predicted characters (argmax along the second dimension)\n",
        "        predicted_chars = torch.argmax(outputs, dim=1)  # Use dim=1 for the second dimension\n",
        "        true_chars = targets\n",
        "\n",
        "        # Count correct predictions\n",
        "        correct_predictions = (predicted_chars == true_chars)\n",
        "        total_correct += correct_predictions.sum().item()\n",
        "        \n",
        "        # Add to loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / len(dataloader) / batch_size\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "end_time = time.time()  # Record the end time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f'Training took {elapsed_time} seconds.')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'char_transformer_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2-HaLXngx221",
      "metadata": {
        "id": "2-HaLXngx221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 26.85%\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model\n",
        "model = CharTransformer(input_size, output_size, embed_size, num_heads, num_layers)\n",
        "model.load_state_dict(torch.load('char_transformer_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Create the test dataset and DataLoader\n",
        "test_dataset = CharDataset(X_test, Y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Evaluation loop\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs, targets = batch\n",
        "\n",
        "        # Convert inputs to PyTorch tensor\n",
        "        input_tensor = torch.as_tensor(inputs, dtype=torch.long).clone().detach()\n",
        "        target_tensor = torch.as_tensor(targets, dtype=torch.long).clone().detach()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_tensor)\n",
        "\n",
        "        # Get predicted characters (argmax along the second dimension)\n",
        "        predicted_chars = torch.argmax(outputs, dim=1)  # Use dim=1 for the second dimension\n",
        "        true_chars = targets\n",
        "\n",
        "        # Count correct predictions\n",
        "        correct_predictions = (predicted_chars == true_chars)\n",
        "        total_correct += correct_predictions.sum().item()\n",
        "        total_samples += correct_predictions.size(0)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = total_correct / total_samples\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad8873e7-d9a1-4406-88be-74a503c89b1c",
      "metadata": {
        "id": "ad8873e7-d9a1-4406-88be-74a503c89b1c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
